# Filebeat + Kafka + Elasticsearch + Kibana 实现日志收集与管理

上篇文章介绍了 [如何在 Django 中优雅的记录日志](<https://github.com/yongxinz/tech-blog/blob/master/django/Django%20%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E7%9A%84%E8%AE%B0%E5%BD%95%E6%97%A5%E5%BF%97.md>)，这篇来谈谈怎么管理以及查看日志。

说到查看日志，这么简单的事情还值得写篇文章吗？文件已经有了，直接 vim 打开不就完了。话虽如此，但有时候这样做却并不可行。

如果是单台服务器的话，直接查看本地文件也是可以的，再搭配一些 Linux 命令，基本可以快速定位到问题。但现实情况中，我们的服务大部分都是部署在多台服务器上的，如果出现故障，到底是哪台服务器出的问题呢？很难排查，只能逐个登录服务器查看日志，这样做效率就太低了。

所以，必须得有一个集中管理日志的地方，可以把多台服务器上的日志汇总到一起。这样如果出现故障，我们到日志集中管理平台上一查，就可以快速定位到问题，并且可以确切的知道具体是哪台服务器出现了问题，何乐而不为呢？

这篇文章主要就是来解决这个问题。

怎么解决呢？说来也简单，因为已经有一套非常成熟的日志分析框架了，叫 ELK，而且在各大互联网公司都有成功的应用实践，网上资料也很多。

由于公司里面已经有一套日志分析框架了，所以，这件事情对我来说就更简单了，我只需要把日志内容发出去就好了。

在这里，我没有用 Logstash，而是用了更轻量的 Filebeat，配置起来也更方便。

**Filebeat 日志源配置：**

```yml
filebeat.inputs:

# Each - is an input. Most options can be set at the input level, so
# you can use different inputs for various configurations.
# Below are the input specific configurations.

- type: log

  # Change to true to enable this input configuration.
  enabled: true

  # Paths that should be crawled and fetched. Glob based paths.
  paths:
    - /log/error.log

	# 这三行可以将 json 内容解析成键值对的形式，否则会将全部 json 内容放到一个 message 字段里
  json.keys_under_root: true
  json.add_error_key: true
  json.overwrite_keys: true
```

**Filebeat 发送到 Elasticsearch：**

```yml
#==================== Elasticsearch template setting ==========================
setup.template.name: "weblog"
setup.template.pattern: "weblog_*"
setup.template.overwrite: false
setup.template.enabled: true
setup.template.settings:
  index.number_of_shards: 1

#-------------------------- Elasticsearch output ------------------------------
output.elasticsearch:
  hosts: ["127.0.0.1:9200"]
  # 按月建索引
  index: "weblog_%{+YYYY.MM}"

  # Protocol - either `http` (default) or `https`.
  # protocol: "https"

  # Authentication credentials - either API key or username/password.
  # api_key: "id:api_key"
  username: "elastic"
  password: "changeme"
```

配置发送到 Elasticsearch 时候出现一个问题，花了很长时间才解决，问题如下：

```shell
(status=404): {"type":"type_missing_exception","reason":"type[doc] missing","index_uuid":"j9yKwou6QDqwEdhn4ZfYmQ","index":"secops-seclog_2020.04.16","caused_by":{"type":"illegal_state_exception","reason":"trying to auto create mapping, but dynamic mapping is disabled"}}
```

网上查找资料，大部分给出的解决办法都是配置 `document_type`，但是我用的 Filebeat 是 5.6 版本，这个参数已经取消了，无奈只能另找办法。

最后，就在我已经快要放弃的时候，通过把 Elasticsearch template type 改成 `doc`，而不是用自定义字段，解决了这个问题。

而且我发现一个很奇怪的现象，那就是总能在快要放弃之时找到解决问题的办法，所以多坚持一下还是很有必要的。

发送到 Elasticsearch 之后，就可以通过 Kibana 从页面来查询数据了，但这样做并不是最好的办法。更通用的架构是，先把数据发送到数据总线 Kafka，然后通过消费者程序来消费 Kafka 里的数据，最后入库到 Elasticsearch 或者其他存储组件。

**Filebeat 发送到 Kafka：**

```yml
output.kafka:
  hosts: ["kafka1:9092"]
  topic: 'web-log'
  username: 'XXX'
  password: 'XXX'
  partition.round_robin:
    reachable_only: false

  required_acks: 1
  compression: gzip
  max_message_bytes: 1000000
```

**去除不需要的字段：**

Filebeat 在发送日志时，会添加一些字段，如果不想要这些字段的话，可以通过下面的配置将这些字段过滤。

```yml
#================================ Processors =====================================

# Configure processors to enhance or manipulate events generated by the beat.

processors:
  - drop_fields:
      fields: ["agent",  "ecs", "host", "input", "log"]
  # - add_host_metadata: ~
  # - add_cloud_metadata: ~
  # - add_docker_metadata: ~
  # - add_kubernetes_metadata: ~
```

以上是 Filebeat 的全部配置，如果想在生产环境使用 ELK 全部组件，那估计还是需要借助于公司基础大数据平台，而且搭建，部署，测试，优化也是一个漫长的过程，我这方面经验也不多，就没办法深入介绍了。

但如果想自己搭建一套测试玩玩，还是比较简单的，直接在官网查询相应文档，配置起来也比较简单。网速给力的话，应该很快就可以搞定，祝大家玩的愉快。

以上。



**参考文档：** 

https://www.elastic.co/guide/index.html